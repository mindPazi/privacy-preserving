{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy-Preserving Techniques for LLM Code Completion\n",
    "\n",
    "This notebook measures and plots the privacy-utility trade-off for code completion using the HumanEval dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.data import HumanEvalDataLoader\n",
    "from src.obfuscation import LowObfuscator, HighObfuscator\n",
    "from src.models import CodeCompletionModel\n",
    "from src.evaluation import UtilityEvaluator, PrivacyEvaluator\n",
    "from src.visualization import PrivacyUtilityPlotter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load HumanEval Dataset\n",
    "\n",
    "Load the first 20 examples from the openai/openai_humaneval test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = HumanEvalDataLoader(num_examples=20)\n",
    "data_loader.load_dataset()\n",
    "\n",
    "prompts = data_loader.get_prompts()\n",
    "canonical_solutions = data_loader.get_canonical_solutions()\n",
    "\n",
    "print(f\"Loaded {len(prompts)} examples\")\n",
    "print(f\"\\nExample prompt (first 300 chars):\\n{prompts[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Obfuscation Functions\n",
    "\n",
    "Two levels of obfuscation:\n",
    "- **Low**: Rename variables to generic names (var1, var2, etc.)\n",
    "- **High**: Replace all identifiers with placeholders and strip comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_obfuscator = LowObfuscator()\n",
    "high_obfuscator = HighObfuscator()\n",
    "\n",
    "low_obfuscated_prompts = [low_obfuscator.obfuscate(p) for p in prompts]\n",
    "high_obfuscated_prompts = [high_obfuscator.obfuscate(p) for p in prompts]\n",
    "\n",
    "print(\"Original prompt (first 200 chars):\")\n",
    "print(prompts[0][:200])\n",
    "print(\"\\nLow obfuscated (first 200 chars):\")\n",
    "print(low_obfuscated_prompts[0][:200])\n",
    "print(\"\\nHigh obfuscated (first 200 chars):\")\n",
    "print(high_obfuscated_prompts[0][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Code Completion Model\n",
    "\n",
    "Using Salesforce/codet5-small for code completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CodeCompletionModel(model_name=\"Salesforce/codet5-small\", device=\"cpu\")\n",
    "model.load_model()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Completions\n",
    "\n",
    "Generate 60 completions total: 20 original, 20 low-obfuscated, 20 high-obfuscated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating completions for original prompts...\")\n",
    "original_completions = model.generate_completions_batch(prompts)\n",
    "\n",
    "print(\"Generating completions for low-obfuscated prompts...\")\n",
    "low_completions = model.generate_completions_batch(low_obfuscated_prompts)\n",
    "\n",
    "print(\"Generating completions for high-obfuscated prompts...\")\n",
    "high_completions = model.generate_completions_batch(high_obfuscated_prompts)\n",
    "\n",
    "print(f\"\\nGenerated {len(original_completions) + len(low_completions) + len(high_completions)} total completions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Utility Scores\n",
    "\n",
    "Using ROUGE-L F1 score to compare completions to canonical solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_evaluator = UtilityEvaluator()\n",
    "\n",
    "original_utility = [utility_evaluator.get_utility_score(c, r) for c, r in zip(original_completions, canonical_solutions)]\n",
    "low_utility = [utility_evaluator.get_utility_score(c, r) for c, r in zip(low_completions, canonical_solutions)]\n",
    "high_utility = [utility_evaluator.get_utility_score(c, r) for c, r in zip(high_completions, canonical_solutions)]\n",
    "\n",
    "print(f\"Original - Mean Utility: {np.mean(original_utility):.4f}, Std: {np.std(original_utility):.4f}\")\n",
    "print(f\"Low Obf  - Mean Utility: {np.mean(low_utility):.4f}, Std: {np.std(low_utility):.4f}\")\n",
    "print(f\"High Obf - Mean Utility: {np.mean(high_utility):.4f}, Std: {np.std(high_utility):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Privacy Scores\n",
    "\n",
    "Using normalized Levenshtein distance between obfuscated and original prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_evaluator = PrivacyEvaluator()\n",
    "\n",
    "original_privacy = [0.0] * len(prompts)\n",
    "low_privacy = privacy_evaluator.compute_privacy_batch(prompts, low_obfuscated_prompts)\n",
    "high_privacy = privacy_evaluator.compute_privacy_batch(prompts, high_obfuscated_prompts)\n",
    "\n",
    "print(f\"Original - Mean Privacy: {np.mean(original_privacy):.4f}, Std: {np.std(original_privacy):.4f}\")\n",
    "print(f\"Low Obf  - Mean Privacy: {np.mean(low_privacy):.4f}, Std: {np.std(low_privacy):.4f}\")\n",
    "print(f\"High Obf - Mean Privacy: {np.mean(high_privacy):.4f}, Std: {np.std(high_privacy):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Scatter Plot\n",
    "\n",
    "Visualize the privacy-utility trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_privacy = original_privacy + low_privacy + high_privacy\n",
    "all_utility = original_utility + low_utility + high_utility\n",
    "all_labels = ['None'] * 20 + ['Low'] * 20 + ['High'] * 20\n",
    "\n",
    "plotter = PrivacyUtilityPlotter(figure_size=(10, 8))\n",
    "fig = plotter.create_scatter_plot(\n",
    "    all_privacy,\n",
    "    all_utility,\n",
    "    labels=all_labels,\n",
    "    title=\"Privacy-Utility Trade-off by Obfuscation Level\",\n",
    "    xlabel=\"Privacy Score (Normalized Levenshtein Distance)\",\n",
    "    ylabel=\"Utility Score (ROUGE-L F1)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analysis\n",
    "\n",
    "Brief analysis of the observed privacy-utility trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PRIVACY-UTILITY TRADE-OFF ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'Level':<10} {'Privacy Mean':>15} {'Utility Mean':>15}\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'None':<10} {np.mean(original_privacy):>15.4f} {np.mean(original_utility):>15.4f}\")\n",
    "print(f\"{'Low':<10} {np.mean(low_privacy):>15.4f} {np.mean(low_utility):>15.4f}\")\n",
    "print(f\"{'High':<10} {np.mean(high_privacy):>15.4f} {np.mean(high_utility):>15.4f}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"1. No obfuscation (None) has privacy=0 since prompts are unchanged.\")\n",
    "print(\"2. Low obfuscation provides moderate privacy with variable renaming.\")\n",
    "print(\"3. High obfuscation maximizes privacy but may reduce utility.\")\n",
    "print(\"4. The trade-off shows that increased privacy often comes at the cost\")\n",
    "print(\"   of reduced utility, as the model receives less semantic information.\")\n",
    "\n",
    "privacy_increase = np.mean(high_privacy) - np.mean(original_privacy)\n",
    "utility_change = np.mean(high_utility) - np.mean(original_utility)\n",
    "print(f\"\\nHigh vs None obfuscation:\")\n",
    "print(f\"  Privacy increase: +{privacy_increase:.4f}\")\n",
    "print(f\"  Utility change: {utility_change:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\n",
    "    'none': {'privacy_scores': original_privacy, 'utility_scores': original_utility},\n",
    "    'low': {'privacy_scores': low_privacy, 'utility_scores': low_utility},\n",
    "    'high': {'privacy_scores': high_privacy, 'utility_scores': high_utility}\n",
    "}\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "plotter.save_figure(fig, 'privacy_utility_scatter.png')\n",
    "print(\"Results saved to results.json and privacy_utility_scatter.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
